# -*- coding: utf-8 -*-
"""Codebase_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SAUz0MaQnKvDqHqg_FQEgJLU9Un-C6Tt


![Screenshot 2024-11-25 at 7 12 58 PM](https://github.com/user-attachments/assets/48dd9de1-b4d2-4318-8f52-85ec209d8ebc)

# Install Necessary Libraries
"""


from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
from langchain_pinecone import PineconeVectorStore
from langchain.embeddings import OpenAIEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
#from google.colab import userdata
from pinecone import Pinecone
import os
import tempfile
from github import Github, Repository
from git import Repo
from openai import OpenAI
from pathlib import Path
from langchain.schema import Document
from pinecone import Pinecone
import re
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

"""# Clone a GitHub Repo locally"""

while True:
       github_repo = input("Enter the GitHub repository URL (e.g., https://github.com/user/repo): ")

       if re.match(r"^https://github\.com/[^/]+/[^/]+$", github_repo):
           break  # Exit the loop if the input is a valid GitHub URL
       else:
           print("Invalid GitHub URL. Please enter a URL in the format 'https://github.com/user/repo'.")

namespace = github_repo.split("/")[-1]

def clone_repository(repo_url):
    """Clones a GitHub repository to a temporary directory.

    Args:
        repo_url: The URL of the GitHub repository.

    Returns:
        The path to the cloned repository.
    """

    repo_name = github_repo.split("/")[-1]
    repo_path = f"/content/{repo_name}"
    Repo.clone_from(repo_url, str(repo_path))
    return repo_path

path = clone_repository(github_repo)



"""# Define which types of files to parse and which files / folders to ignore"""

SUPPORTED_EXTENSIONS = {'.py', '.js', '.tsx', '.jsx', '.ipynb', '.java',
                         '.cpp', '.ts', '.go', '.rs', '.vue', '.swift', '.c', '.h'}

IGNORED_DIRS = {'node_modules', 'venv', 'env', 'dist', 'build', '.git',
                '__pycache__', '.next', '.vscode', 'vendor'}

def get_file_content(file_path, repo_path):
    """
    Get content of a single file.

    Args:
        file_path (str): Path to the file

    Returns:
        Optional[Dict[str, str]]: Dictionary with file name and content
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()

        rel_path = os.path.relpath(file_path, repo_path)

        return {
            "name": rel_path,
            "content": content
        }

    except Exception as e:
        print(f"Error processing file {file_path}: {str(e)}")
        return None



def get_main_files_content(repo_path: str):
    """
    Get content of supported code files from the local repository.

    Args:
        repo_path: Path to the local repository

    Returns:
        List of dictionaries containing file names and contents
    """

    files_content = []

    try:

        for root, _, files in os.walk(repo_path):
            if any(ignored_dir in root for ignored_dir in IGNORED_DIRS):
                continue

            for file in files:
                file_path = os.path.join(root, file)
                if os.path.splitext(file)[1] in SUPPORTED_EXTENSIONS:
                    file_content = get_file_content(file_path, repo_path)

                    if file_content:
                        files_content.append(file_content)

    except Exception as e:
        print(e)

    return files_content

files_content = get_main_files_content(path)

files_content

"""# Embeddings"""

def get_huggingface_embeddings(text, model_name="sentence-transformers/all-mpnet-base-v2"):
    model = SentenceTransformer(model_name)
    return model.encode(text)

text = "I like coding"

embeddings = get_huggingface_embeddings(text)

embeddings



"""# Setting up Pinecone
**1. Create an account on [Pinecone.io](https://app.pinecone.io/)**

**2. Create a new index called "codebase-rag" and set the dimensions to 768. Leave the rest of the settings as they are.**

![Screenshot 2024-11-24 at 10 58 50 PM](https://github.com/user-attachments/assets/f5fda046-4087-432a-a8c2-86e061005238)



**3. Create an API Key for Pinecone**

![Screenshot 2024-11-24 at 10 44 37 PM](https://github.com/user-attachments/assets/e7feacc6-2bd1-472a-82e5-659f65624a88)


**4. Store your Pinecone API Key within Google Colab's secrets section, and then enable access to it (see the blue checkmark)**

![Screenshot 2024-11-24 at 10 45 25 PM](https://github.com/user-attachments/assets/eaf73083-0b5f-4d17-9e0c-eab84f91b0bc)


"""

# Replace colab secrets with os.getenv
pinecone_api_key = os.getenv("PINECONE_API_KEY")
os.environ['PINECONE_API_KEY'] = pinecone_api_key

# Initialize Pinecone
pc = Pinecone(api_key=pinecone_api_key,)

# Connect to your Pinecone index
pinecone_index = pc.Index("codebase-rag")

vectorstore = PineconeVectorStore(index_name="codebase-rag", embedding=HuggingFaceEmbeddings())

# Insert the codebase embeddings into Pinecone

documents = []

for file in files_content:
    doc = Document(
        page_content=f"{file['name']}\n\n{file['content']}",
        metadata={"source": file['name']}
    )

    documents.append(doc)


vectorstore = PineconeVectorStore.from_documents(
    documents=documents,
    embedding=HuggingFaceEmbeddings(),
    index_name="codebase-rag",
    namespace=namespace
)

documents

"""OpenRouter is a Platform for using multiple LLMs using single API Key

# Perform RAG
1. Get your OpenRouter API Key [here](https://openrouter.ai/settings/keys)

2. Paste your OpenRouter Key into your Google Colab secrets, and make sure to enable permissions for it

![Image](https://github.com/user-attachments/assets/bd64c5aa-952e-4a1e-9ac0-01d8fe93aaa1)
"""

# Replace colab secrets with os.getenv
client = OpenAI(
    base_url="https://openrouter.ai/api/v1",
    api_key=os.getenv("OPENROUTER_API_KEY")
)

query = "How is the content generating"

raw_query_embedding = get_huggingface_embeddings(query)

raw_query_embedding

top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(),
                                   top_k=5,
                                   include_metadata=True,
                                   namespace=namespace)

top_matches

context = [item['metadata']['text'] for item in top_matches['matches']]

context

augmented_query = "<CONTEXT>\n" + "\n\n-------\n\n".join(context) + "\n-------\n</CONTEXT>\n\n\n\nMY QUESTION:\n" + query

print(augmented_query)

from google import genai
from google.colab import userdata


# Replace colab secrets with os.getenv
GOOGLE_API_KEY=os.getenv('GOOGLE_API_KEY')
client = genai.Client(api_key=GOOGLE_API_KEY)

from google import genai

client = genai.Client(api_key=GOOGLE_API_KEY)
chat = client.chats.create(model="gemini-2.0-flash")

response = chat.send_message("How does Email is generated")
print(response.text)

for message in chat.get_history():
    print(f'role - {message.role}',end=": ")
    print(message.parts[0].text)

from openai import OpenAI

client = OpenAI(
    api_key=GOOGLE_API_KEY,
    base_url="https://generativelanguage.googleapis.com/v1beta/openai/"
)

response = client.chat.completions.create(
    model="gemini-2.0-flash",
    n=1,
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {
            "role": "user",
            "content": "Explain to me how AI works"
        }
    ]
)

print(response.choices[0].message)

"""#DeepSeek"""
DEEPSEEK_API_KEY=os.getenv('DEEPSEEK_API_KEY')

from openai import OpenAI

client = OpenAI(api_key="", base_url="https://api.deepseek.com")

response = client.chat.completions.create(
    model="deepseek-chat",
    messages=[
        {"role": "system", "content": "You are a helpful assistant"},
        {"role": "user", "content": "Hello"},
    ],
    stream=False
)

print(response.choices[0].message.content)

"""Multi-Models"""

system_prompt = """You are a Senior Software Engineer,

Answer the question I have about the codebase based on the context provided. Always consider all of the context provided
to answer my question.
"""


llm_response = client.chat.completions.create(
    model="openai/gpt-4o",
    extra_body={
        "models": ["anthropic/claude-3.5-sonnet", "gryphe/mythomax-l2-13b"],
    },
    messages=[
        {
            "role": "user",
            "content": "What is the meaning of building this project?"
        }
    ]
)

response = llm_response.choices[0].message.content

def perform_rag(query, model="gemini-2.0-flash"):
    raw_query_embedding = get_huggingface_embeddings(query)

    top_matches = pinecone_index.query(vector=raw_query_embedding.tolist(), top_k=5, include_metadata=True, namespace=namespace)

    # Get the list of retrieved texts
    contexts = [item['metadata']['text'] for item in top_matches['matches']]

    augmented_query = "<CONTEXT>\n" + "\n\n-------\n\n".join(contexts[:10]) + "\n-------\n</CONTEXT>\n\n\n\nMY QUESTION:\n" + query

    # Modify the prompt below as need to improve the response quality
    system_prompt = f"""You are a Senior Software Engineer,\n\n    Answer any questions I have about the codebase, based on the code provided. Always consider all of the context provided when forming a response.\n    """

    llm_response = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": augmented_query}
        ]
    )

    if llm_response and llm_response.choices:
        return llm_response.choices[0].message.content
    else:
        print("Error: Language model API did not return any valid responses.")  # Print error for debugging
        return "No response from language model." # Or handle the error in another suitable way

print(response)

"""RAG using Gemini for Text Completions"""



response = perform_rag("explain working of each files in detail")
from IPython.display import Markdown, display


display(Markdown(response))

display(Markdown(response))

response = perform_rag("What does this github repo do?", "anthropic/claude-3.7-sonnet:thinking")
from IPython.display import Markdown, display

display(Markdown(response))

app = FastAPI()

class AnalyzeRequest(BaseModel):
    github_url: str
    analysis_type: str

@app.post("/analyze/code-review")
async def analyze_code_review(request: AnalyzeRequest):
    query = f"Perform a code review for the repository at {request.github_url}."
    response = perform_rag(query)
    return {"message": "Code review analysis completed.", "response": response}

@app.post("/analyze/complexity")
async def analyze_complexity(request: AnalyzeRequest):
    query = f"Analyze the code complexity for the repository at {request.github_url}."
    response = perform_rag(query)
    return {"message": "Complexity analysis completed.", "response": response}

@app.post("/analyze/dependencies")
async def analyze_dependencies(request: AnalyzeRequest):
    query = f"Analyze the dependencies for the repository at {request.github_url}."
    response = perform_rag(query)
    return {"message": "Dependency analysis completed.", "response": response}